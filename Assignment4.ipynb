{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "uniform-billy",
   "metadata": {
    "id": "uniform-billy"
   },
   "source": [
    "<h1 align=\"center\"> CSE 242 Assignment 4, Spring 2025\n",
    "\n",
    "    Your name: Mengxiao Hu   Student ID:2172399(mhu110)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508d88e",
   "metadata": {
    "id": "6508d88e"
   },
   "source": [
    "## Instruction \n",
    "\n",
    "- Submit your assignments onto **canvas** by the due date. Upload a <code>zip</code> file containing:\n",
    "\n",
    "    (1) The saved/latest <code>.ipynb</code> file, please **rename this file with your name included**.\n",
    "\n",
    "    (2) Also save your file into a pdf version, if error appears, save an html version instead (easy to grade for written questions).\n",
    "    \n",
    "    **For assignment related questions, please reach TA or grader through Slack/Piazza/Email.**\n",
    "    \n",
    "- This is an **individual** assignment. All help from others (from the web, books other than text, or people other than the TA or instructor) must be clearly acknowledged. \n",
    "\n",
    "## Objective \n",
    "\n",
    "- **Task 1:** EM algorithm (Mathematical Derivation)\n",
    "- **Task 2:** K-Means implementation (Coding)\n",
    "- **Task 3:** Kernel Methods with Noisy Setting (Coding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oBL7VIbHbMhT",
   "metadata": {
    "id": "oBL7VIbHbMhT"
   },
   "source": [
    "# Question 1. (EM algorithm, 20 pts) (the EM proof is for extra credit only)\n",
    "\n",
    "Derive the E-step and M-step update equations of EM algorithm for estimating the Gaussian mixture model $p(X;\\theta) = ∑_{k=1}^{K} \\pi_k N(x; \\mu_k, \\sigma_k^2)$ where $\\pi_k$ is the mixture weight with $\\pi_k \\ge 0$  and  $∑_{k=1}^{K} \\pi_k = 1$, and $\\mu_k$, $\\sigma^2_k$ are the mean and variance of the gaussian distribution corresponding to cluster k. \n",
    "\n",
    "For the E-step, first prove that $z_{ik} = \\color{blue}{P(z_i = k | X, \\mu ,\\sigma, \\pi)} = \\dfrac {\\pi_k N(x_i; \\mu_k, \\sigma_k^2)}{∑_{k=1}^{K} \\pi_k N(x_i; \\mu_k, \\sigma_k^2)}$. Then, for the M-step, show the derivation to compute the updates for $(\\mu_k, \\pi_k)$. Note that, you don't need to show the derivation for $\\sigma_k$. For each derivation step, mention the concept applied (e.g. just 2-3 keywords, e.g. formula for expectation, independence of datapoints, (f+g)' = f' + g', etc ... ). \n",
    "\n",
    "**Hint:** For the M-step, you need to solve for $\\mu_k^t = \\underset{\\mu_k}{argmax} \\ E_{\\color{blue}{p(Z | X, \\mu^{(t-1)}, \\sigma^{(t-1)}, \\pi^{(t-1)})}}{[\\log p(X, Z | \\mu, \\sigma, \\pi)]}$ (and similarly for $\\pi_k$) by applying the first order conditions for function optimization (take derivative and set it to zero). Note that the term $\\color{blue}{p(Z|X,\\mu^{(t-1)},\\sigma^{(t-1)},\\pi^{(t-1)})}$ is the one computed in the E-step, and uses fixed values for $\\mu, \\sigma, \\pi$ from the previous iteration (t-1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81f524",
   "metadata": {},
   "source": [
    "## E-step Derivation\n",
    "\n",
    "By Bayes' rule:\n",
    "$$z_{ik} = P(z_i = k | x_i, \\mu, \\sigma, \\pi) = \\frac{P(x_i | z_i = k, \\mu, \\sigma, \\pi) \\cdot P(z_i = k | \\mu, \\sigma, \\pi)}{P(x_i | \\mu, \\sigma, \\pi)}$$\n",
    "\n",
    "Given $z_i = k$, $x_i$ comes from the $k$-th Gaussian component:\n",
    "$$P(x_i | z_i = k, \\mu, \\sigma, \\pi) = N(x_i; \\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "The prior probability of $z_i = k$ is the mixture weight:\n",
    "$$P(z_i = k | \\mu, \\sigma, \\pi) = \\pi_k$$\n",
    "\n",
    "For the denominator, by the law of total probability:\n",
    "$$P(x_i | \\mu, \\sigma, \\pi) = \\sum_{j=1}^{K} P(x_i | z_i = j, \\mu, \\sigma, \\pi) \\cdot P(z_i = j | \\mu, \\sigma, \\pi) = \\sum_{j=1}^{K} \\pi_j N(x_i; \\mu_j, \\sigma_j^2)$$\n",
    "\n",
    "Substituting these into the Bayes' formula:\n",
    "$$z_{ik} = \\frac{\\pi_k N(x_i; \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{K} \\pi_j N(x_i; \\mu_j, \\sigma_j^2)}$$\n",
    "\n",
    "## M-step Derivation\n",
    "\n",
    "The complete log-likelihood is:\n",
    "\n",
    "$$\\log p(X, Z | \\mu, \\sigma, \\pi) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} 1(z_i = k) \\log[\\pi_k N(x_i; \\mu_k, \\sigma_k^2)]$$\n",
    "\n",
    "Taking the expectation with respect to $p(Z | X, \\mu^{(t-1)}, \\sigma^{(t-1)}, \\pi^{(t-1)})$:\n",
    "\n",
    "$$\\sum_{i=1}^{N} \\sum_{k=1}^{K} E[1(z_i = k)] \\log[\\pi_k N(x_i; \\mu_k, \\sigma_k^2)] = \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik}^{(t)} \\log[\\pi_k N(x_i; \\mu_k, \\sigma_k^2)] = \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik}^{(t)} \\log \\pi_k + \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik}^{(t)} \\log N(x_i; \\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "### Updating $\\pi_k$\n",
    "\n",
    "Using the Lagrangian method with multiplier $\\lambda$:\n",
    "$$L(\\pi, \\lambda) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik}^{(t)} \\log \\pi_k - \\lambda \\left(\\sum_{k=1}^{K} \\pi_k - 1\\right)$$\n",
    "\n",
    "Taking the derivative on $\\pi_k$ and setting to zero:\n",
    "$$\\frac{\\partial L}{\\partial \\pi_k} = \\sum_{i=1}^{N} \\frac{z_{ik}^{(t)}}{\\pi_k} - \\lambda = 0$$\n",
    "\n",
    "This is equivalent to:\n",
    "$$N = \\sum_{k=1}^{K} \\sum_{i=1}^{N} z_{ik}^{(t)} = \\lambda \\sum_{k=1}^{K} \\pi_k = \\lambda \\cdot 1 = \\lambda$$\n",
    "\n",
    "Therefore:\n",
    "$$\\pi_k^{(t)} = \\frac{1}{N} \\sum_{i=1}^{N} z_{ik}^{(t)}$$\n",
    "### Updating $\\mu_k$\n",
    "\n",
    "Taking the derivative on $\\mu_k$:\n",
    "$$\\frac{\\partial}{\\partial \\mu_k} \\sum_{i=1}^{N} z_{ik}^{(t)} \\log N(x_i; \\mu_k, \\sigma_k^2) = 0$$\n",
    "\n",
    "Expanding the normal distribution:\n",
    "$$\\log N(x_i; \\mu_k, \\sigma_k^2) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma_k) - \\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2}$$\n",
    "\n",
    "Substituting into the derivation$:\n",
    "$$\\sum_{i=1}^{N} z_{ik}^{(t)} \\frac{(x_i - \\mu_k)}{\\sigma_k^2} = 0$$\n",
    "\n",
    "Solving for $\\mu_k$:\n",
    "$$\\sum_{i=1}^{N} z_{ik}^{(t)} x_i = \\mu_k \\sum_{i=1}^{N} z_{ik}^{(t)}$$\n",
    "\n",
    "Therefore:\n",
    "$$\\mu_k^{(t)} = \\frac{\\sum_{i=1}^{N} z_{ik}^{(t)} x_i}{\\sum_{i=1}^{N} z_{ik}^{(t)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ICa8rEI9qp5B",
   "metadata": {
    "id": "ICa8rEI9qp5B"
   },
   "source": [
    "# Question 2. (K-Means implementation, 20 pts)\n",
    "\n",
    "#### **Question 2.1.** Implement K-means in Python from scratch. Complete following sub-functions `update_centroids` and `update_assignments`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W3VcW7Z_XDhH",
   "metadata": {
    "id": "W3VcW7Z_XDhH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_assignments(data, centroids):\n",
    "\n",
    "  ########################\n",
    "  #### YOUR CODE HERE ####\n",
    "\n",
    "  ## you will get cluster# \n",
    "  ##assignments here #####\n",
    "  ########################\n",
    "\n",
    "  n_samples = data.shape[0]\n",
    "  k = centroids.shape[0]\n",
    "    \n",
    "  distances = np.zeros((n_samples, k))\n",
    "    \n",
    "  for i in range(k):\n",
    "    distances[:, i] = np.sum((data - centroids[i])**2, axis=1)\n",
    "    \n",
    "  assignments = np.argmin(distances, axis=1)\n",
    "    \n",
    "  return assignments\n",
    "\n",
    "def update_centroids(data,centroids,assignments):\n",
    "\n",
    "  ########################\n",
    "  #### YOUR CODE HERE ####\n",
    "  ########################\n",
    "\n",
    "  k = centroids.shape[0]\n",
    "    \n",
    "  for i in range(k):\n",
    "    mask = (assignments == i)\n",
    "    if np.any(mask):\n",
    "      centroids[i] = np.mean(data[mask], axis=0)\n",
    "    \n",
    "  return centroids\n",
    "\n",
    "\n",
    "def kmeans(data, centroids, max_iterations = 100):    \n",
    "\n",
    "    for j in range(max_iterations):\n",
    "        # update cluter assignments\n",
    "        assignments = update_assignments(data,centroids)    # WRITE CODE FOR update_assignments\n",
    "        \n",
    "        # update centroid locations\n",
    "        centroids = update_centroids(data,centroids,assignments)  # WRITE CODE FOR update_centroids\n",
    "        \n",
    "    # final assignment update\n",
    "    assignments = update_assignments(data,centroids)\n",
    "    return centroids, assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FbamChYmYDY2",
   "metadata": {
    "id": "FbamChYmYDY2"
   },
   "source": [
    "#### **Question 2.2.** Run your code on following toy dataset for different k-values, where k = {2, 3, 4, 6, 10} and plot the cluster assignments for different k's as shown in following diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KqwqiSdlXq_x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1667362337596,
     "user": {
      "displayName": "Fatemeh Elyasi",
      "userId": "06029049739695213247"
     },
     "user_tz": 420
    },
    "id": "KqwqiSdlXq_x",
    "outputId": "931705cd-0c0f-4079-d052-0a100f86e10c"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "n_samples = 4000\n",
    "n_components = 4\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=n_samples, centers=n_components, cluster_std=0.60, random_state=0\n",
    ")\n",
    "\n",
    "colors = [\"#4EACC5\", \"#FF9C34\", \"#4E9A06\", \"m\"]\n",
    "\n",
    "for k, col in enumerate(colors):\n",
    "    cluster_data = y_true == k\n",
    "    plt.scatter(X[cluster_data, 0], X[cluster_data, 1], c=col, marker=\".\", s=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xw9ab012ZjsH",
   "metadata": {
    "id": "xw9ab012ZjsH"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# function to get initial cluster centroids; we randomly choose k points from the dataset \n",
    "def get_initial_clusters(k, X):\n",
    "  random_indices = np.random.randint(0, X.shape[0], k)\n",
    "  initial_centroids = X[random_indices]\n",
    "\n",
    "  return initial_centroids\n",
    "\n",
    "k_values = [2, 3, 4, 6, 10]\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "  \n",
    "  centroids = get_initial_clusters(k, X)\n",
    "  centroids, assignments = kmeans(X, centroids)\n",
    "    \n",
    "  plt.subplot(2, 3, i+2)\n",
    "  plt.title(f'K-Means (k={k})')\n",
    "    \n",
    "  from matplotlib.cm import get_cmap\n",
    "  cmap = get_cmap('tab10', k)\n",
    "    \n",
    "  for j in range(k):\n",
    "    cluster_points = X[assignments == j]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], color=cmap(j), marker=\".\", s=10)\n",
    "    \n",
    "  plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"x\", s=100, linewidths=3, color=\"k\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13a1da5",
   "metadata": {
    "id": "b13a1da5"
   },
   "source": [
    "# Question 3. (Kernel Methods with Noisy Setting, 60 pts)\n",
    "\n",
    "**SVM on synthetic dataset generated as follows:**\n",
    "\n",
    "- Draw $1000\\ (x_0, x_1)$ feature vectors from the 2-D Gaussian distribution with mean $\\mu_+ = (1,1)$ and $\\Sigma_+ = [1, 0; 0, 1]$ and label them as $+1$.\n",
    "\n",
    "- Draw $1000\\ (x_0, x_1)$ feature vectors from the 2-D Gaussian distribution with mean $\\mu_- = (-1,-1)$ and $\\Sigma_- = [3, 0; 0, 3]$ and label them as $-1$.\n",
    "    \n",
    "- This gives you a $2000$ example training set. Repeat the above to draw a test set the same way.\n",
    "\n",
    "Use a SVM package ( scikit-learn svm.SVC class) to learn SVMs with a variety of parameter settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e44f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "X_train1 = np.random.multivariate_normal([1, 1], [[1, 0], [0, 1]], 1000)\n",
    "X_train2 = np.random.multivariate_normal([-1, -1], [[3, 0], [0, 3]], 1000)\n",
    "\n",
    "X_test1 = np.random.multivariate_normal([1, 1], [[1, 0], [0, 1]], 1000)\n",
    "X_test2 = np.random.multivariate_normal([-1, -1], [[3, 0], [0, 3]], 1000)\n",
    "\n",
    "Y_train1 = np.ones(1000)\n",
    "Y_train2 = -1*np.ones(1000)\n",
    "\n",
    "Y_test1 = np.ones(1000)\n",
    "Y_test2 = -1*np.ones(1000)\n",
    "\n",
    "X = np.concatenate((X_train1, X_train2), axis=0)\n",
    "y = np.concatenate((Y_train1, Y_train2), axis=0)\n",
    "\n",
    "X_train, y_train = shuffle(X, y, random_state=0)\n",
    "\n",
    "X_test = np.concatenate((X_test1, X_test2), axis=0)\n",
    "Y_test = np.concatenate((Y_test1, Y_test2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96b9f4",
   "metadata": {
    "id": "7c96b9f4"
   },
   "source": [
    "## (a -- 20 pts) \n",
    "\n",
    "- Use an RBF kernel with parameters $C=1$, $\\gamma = 0.01$. \n",
    "\n",
    "- For each training data with +1 label, randomly flip their label to -1 with probability $\\textbf{0.35}$.\n",
    "\n",
    "- For each training data with -1 label, randomly flip their label to +1 with probability $\\textbf{0.20}$.\n",
    "\n",
    "- Train with the above noisy training examples.\n",
    "\n",
    "- Random flipping introduces the randomness. You can repeat multiple times (e.g. 20) and then report the average accuracy on the testing dataset (clean) in the noise parameter setting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ca9a5",
   "metadata": {
    "id": "0c9ca9a5"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def add_label_noise(y, flip_pos_prob=0.35, flip_neg_prob=0.20):\n",
    "    y_noisy = y.copy()\n",
    "    \n",
    "    pos_indices = np.where(y == 1)[0]\n",
    "    neg_indices = np.where(y == -1)[0]\n",
    "    \n",
    "    flip_pos = np.random.random(len(pos_indices)) < flip_pos_prob\n",
    "    y_noisy[pos_indices[flip_pos]] = -1\n",
    "    \n",
    "    flip_neg = np.random.random(len(neg_indices)) < flip_neg_prob\n",
    "    y_noisy[neg_indices[flip_neg]] = 1\n",
    "    \n",
    "    return y_noisy\n",
    "\n",
    "n_repetitions = 20\n",
    "accuracies = []\n",
    "\n",
    "for i in range(n_repetitions):\n",
    "    y_train_noisy = add_label_noise(y_train)\n",
    "    \n",
    "    svm_model = SVC(kernel='rbf', C=1.0, gamma=0.01)\n",
    "    svm_model.fit(X_train, y_train_noisy)\n",
    "    \n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Repetition {i+1}: Test accuracy = {accuracy:.4f}\")\n",
    "\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "\n",
    "print(f\"\\nResults for Task 1:\")\n",
    "print(f\"SVM with RBF kernel (C=1, gamma=0.01) on noisy data\")\n",
    "print(f\"Average test accuracy over {n_repetitions} repetitions: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a1729",
   "metadata": {
    "id": "e78a1729"
   },
   "source": [
    "## (b -- 20 pts) Open question\n",
    "\n",
    "- Try using **K-Nearst Neighbors** to correct wrong labels before training. \n",
    "\n",
    "- Then train the model with the newly processed training dataset. \n",
    "\n",
    "- Report the accuracy on the testing dataset in the noise parameter setting. Do you observe performance improvement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087633b",
   "metadata": {
    "id": "3087633b"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def correct_labels_with_knn(X, y_noisy, k=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, y_noisy)\n",
    "    y_corrected = knn.predict(X)\n",
    "    return y_corrected\n",
    "\n",
    "n_repetitions = 20\n",
    "    \n",
    "accuracies_baseline = []  # SVM with noisy labels (no correction)\n",
    "accuracies_corrected = []  # SVM with KNN-corrected labels\n",
    "    \n",
    "for i in range(n_repetitions):\n",
    "    y_train_noisy = add_label_noise(y_train)\n",
    "        \n",
    "    svm_baseline = SVC(kernel='rbf', C=1.0, gamma=0.01)\n",
    "    svm_baseline.fit(X_train, y_train_noisy)\n",
    "    y_pred_baseline = svm_baseline.predict(X_test)\n",
    "    accuracy_baseline = accuracy_score(Y_test, y_pred_baseline)\n",
    "    accuracies_baseline.append(accuracy_baseline)\n",
    "        \n",
    "    y_train_corrected = correct_labels_with_knn(X_train, y_train_noisy)\n",
    "        \n",
    "    svm_corrected = SVC(kernel='rbf', C=1.0, gamma=0.01)\n",
    "    svm_corrected.fit(X_train, y_train_corrected)\n",
    "    y_pred_corrected = svm_corrected.predict(X_test)\n",
    "    accuracy_corrected = accuracy_score(Y_test, y_pred_corrected)\n",
    "    accuracies_corrected.append(accuracy_corrected)\n",
    "        \n",
    "    print(f\"Repetition {i+1}: Baseline Accuracy = {accuracy_baseline:.4f}, Corrected Accuracy = {accuracy_corrected:.4f}\")\n",
    "\n",
    "avg_accuracy_baseline = np.mean(accuracies_baseline)\n",
    "avg_accuracy_corrected = np.mean(accuracies_corrected)\n",
    "     \n",
    "print(f\"Baseline (noisy labels): {avg_accuracy_baseline:.4f}\")\n",
    "print(f\"KNN-corrected labels: {avg_accuracy_corrected:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879b04d",
   "metadata": {
    "id": "9879b04d"
   },
   "source": [
    "## (c -- 20 pts) Open question\n",
    "\n",
    "- Try using **clustering (i.e., K-means, EM-clustering)** to correct wrong labels before training. \n",
    "\n",
    "- Then train the model with the newly processed training dataset. \n",
    "\n",
    "- Report the accuracy on the testing dataset in the noise parameter setting. Do you observe performance improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b91ea2",
   "metadata": {
    "id": "06b91ea2"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def correct_labels_with_kmeans(X, y_noisy, n_clusters=2):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Map cluster labels to original class labels\n",
    "    clusters = np.unique(cluster_labels)\n",
    "    mapping = {}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        indices = np.where(cluster_labels == cluster)[0]\n",
    "        \n",
    "        labels_in_cluster = y_noisy[indices]\n",
    "        unique_labels, counts = np.unique(labels_in_cluster, return_counts=True)\n",
    "        \n",
    "        if len(unique_labels) == 0:\n",
    "            mapping[cluster] = 1 \n",
    "        else:\n",
    "            majority_idx = np.argmax(counts)\n",
    "            mapping[cluster] = unique_labels[majority_idx]\n",
    "    \n",
    "    y_corrected = np.array([mapping[cluster] for cluster in cluster_labels])\n",
    "    return y_corrected\n",
    "    \n",
    "n_repetitions = 20\n",
    "    \n",
    "accuracies_baseline = []  # SVM with noisy labels (no correction)\n",
    "accuracies_corrected = []  # SVM with K-means labels\n",
    "\n",
    "all_corrected_labels = []\n",
    "    \n",
    "for i in range(n_repetitions):\n",
    "    y_train_noisy = add_label_noise(y_train)\n",
    "        \n",
    "    svm_baseline = SVC(kernel='rbf', C=1.0, gamma=0.01)\n",
    "    svm_baseline.fit(X_train, y_train_noisy)\n",
    "    y_pred_baseline = svm_baseline.predict(X_test)\n",
    "    accuracy_baseline = accuracy_score(Y_test, y_pred_baseline)\n",
    "    accuracies_baseline.append(accuracy_baseline)\n",
    "        \n",
    "    y_train_corrected = correct_labels_with_kmeans(X_train, y_train_noisy)\n",
    "        \n",
    "    svm_corrected = SVC(kernel='rbf', C=1.0, gamma=0.01)\n",
    "    svm_corrected.fit(X_train, y_train_corrected)\n",
    "    y_pred_corrected = svm_corrected.predict(X_test)\n",
    "    accuracy_corrected = accuracy_score(Y_test, y_pred_corrected)\n",
    "    accuracies_corrected.append(accuracy_corrected)\n",
    "        \n",
    "    print(f\"Repetition {i+1}: Baseline Accuracy = {accuracy_baseline:.4f}, Corrected Accuracy = {accuracy_corrected:.4f}\")\n",
    "\n",
    "avg_accuracy_baseline = np.mean(accuracies_baseline)\n",
    "avg_accuracy_corrected = np.mean(accuracies_corrected)\n",
    "     \n",
    "print(f\"Baseline (noisy labels): {avg_accuracy_baseline:.4f}\")\n",
    "print(f\"K-means-corrected labels: {avg_accuracy_corrected:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
